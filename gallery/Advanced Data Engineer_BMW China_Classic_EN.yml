locale:
  language: en

layouts:
  - engine: latex
    sections:
      aliases:
        basics: "Basics"
        education: "Education"
        work: "Work Experience"
        projects: "Projects"
        certificates: "Certificates"
        skills: "Skills"
        languages: "Languages"
        interests: "Interests"
      order:
        - basics
        - education
        - work
        - skills
        - certificates
        - projects
        - languages
        - interests
    page:
      margins:
        top: 1.5cm
        left: 1.5cm
        right: 1.5cm
        bottom: 1.5cm
      showPageNumbers: true
    template: moderncv-classic
    typography:
      fontSize: 11pt
  - engine: markdown
  - engine: html
    template: calm
    typography:
      fontSize: 16px

content:
  basics:
    name: Wang Zhiwei
    phone: "+86 199 8888 8888"
    email: nocoding188@gmail.com
    url: https://zhiweio.me
    summary: |
      - Senior Data Engineer with 6+ years of experience specializing in distributed big data systems and cloud-native architectures, well-positioned to support high-scale Automated Driving (AD) R&D environments.
      - Proficient in AWS and Azure ecosystems, utilizing Spark, Python, and Kubernetes to design robust backend applications and CI/CD workflows for enterprise-grade data processing.
      - Proven ability to lead technical initiatives and define big data strategies, bridging the gap between complex business requirements and efficient, scalable infrastructure solutions.
  profiles:
    - network: GitHub
      url: https://github.com/zhiweio
      username: zhiweio
  education:
    - institution: Suzhou University of Science and Technology
      degree: Bachelor
      area: Computer Science and Technology
      startDate: Sep 1, 2015
      endDate: Jul 1, 2019
      summary: |
        - Awarded Third Prize in the 4th National University Cloud Computing Application Innovation Competition (2018).
        - Project: "Implementation of Large-scale Distributed Functional Dependency Mining Algorithm Based on Spark".
  work:
    - name: Cognizant Technology Solutions Shanghai Co. Ltd.
      url: https://www.cognizant.com
      position: Senior Data Engineer
      startDate: Dec 2022
      endDate:
      summary: |
        - Led the migration of a 10TB+ legacy data warehouse to AWS, utilizing AWS Glue and DMS to modernize infrastructure, resulting in a 40% reduction in query latency and 25% lower maintenance costs.
        - Architected a unified Customer Data Platform (CDP) integrating 8+ disparate systems (Salesforce, Databricks) using Spark and Python, processing billions of records daily to enable a 360-degree customer view.
        - Optimized core data processing workflows in Databricks by implementing advanced partitioning and caching strategies, reducing daily processing time from 6 hours to 2.5 hours (58% improvement).
        - Designed a Master Data Management (MDM) solution on Azure Data Lake to resolve critical retail data inconsistencies, reducing discrepancies by 95% and streamlining operations.
      keywords:
        - "AWS (Glue, DMS)"
        - "Databricks & Spark"
        - "Python"
        - "Cloud Migration"
        - "Data Architecture"
    - name: Patsnap Information Technology (Suzhou) Co. Ltd.
      url: https://www.patsnap.com
      position: Data Engineer
      startDate: Jun 2021
      endDate: Nov 2022
      summary: |
        - Architected a scalable data lake solution on AWS S3 and Athena for 1.8 billion global patent records, orchestrating Spark jobs via DolphinScheduler to support TB-scale financial analysis.
        - Developed a custom "Spark Diff" engine to synchronize 500M+ daily records to TiDB, achieving minute-level processing for complex nested structures and ensuring efficient incremental updates.
        - Built a real-time wide table system using Flink, Kafka, and TiCDC, patching open-source components to enable sub-second data ingestion for enterprise business insights.
        - Designed "Guardian," a continuous delivery tool integrated with GitLab CI/CD, automating Docker deployments and reducing delivery costs for 10+ localized banking projects.
      keywords:
        - "AWS (S3, Athena, EMR)"
        - "Apache Spark & Flink"
        - "Kafka"
        - "CI/CD (GitLab)"
        - "TiDB"
    - name: Intsig Information Co., Ltd.
      url: https://www.intsig.com
      position: Data Engineer
      startDate: Jun 2019
      endDate: Jun 2021
      summary: |
        - Engineered a high-throughput real-time data pipeline using Redis and Kafka, processing over 100 billion data points to ensure sub-second freshness for a platform with 230M+ entities.
        - Architected a Python-based ETL optimization library that standardized incremental update logic across 1000+ dimensions, reducing boilerplate code by 90%.
        - Led the DevOps transformation by migrating from SVN to GitLab and implementing CI/CD pipelines, significantly improving team development efficiency and deployment speed.
      keywords:
        - "Real-time Streaming"
        - "Kafka & Redis"
        - "Python ETL"
        - "DevOps"
        - "Data Modeling"
  skills:
    - name: Cloud & Infrastructure
      level: Expert
      keywords:
        - "AWS (EMR, Glue, Redshift)"
        - "Azure (Synapse, Data Lake)"
        - "Kubernetes & Docker"
        - "Terraform (IaC)"
        - "CI/CD (GitLab, Jenkins)"
    - name: Big Data & Streaming
      level: Expert
      keywords:
        - "Apache Spark & Flink"
        - "Kafka Event Streaming"
        - "Hadoop Ecosystem (Hive)"
        - "Databricks Platform"
    - name: Data Architecture & Engineering
      level: Advanced
      keywords:
        - "Data Warehouse & Lakehouse"
        - "ETL/ELT Pipeline Design"
        - "Serverless Data Processing"
        - "Airflow Orchestration"
    - name: Programming & Languages
      level: Advanced
      keywords:
        - "Python & SQL"
        - "Java & Scala"
        - "Shell Scripting"
    - name: Database Systems
      level: Advanced
      keywords:
        - "PostgreSQL & MySQL"
        - "NoSQL (MongoDB, Redis)"
        - "TiDB (NewSQL)"
        - "SQL Server"
  certificates:
    - name: AWS Certified Data Analytics â€“ Specialty
      url:
      issuer: "AWS"
      date: "Jan, 2024"
    - name: AWS Certified Developer - Associate
      url:
      issuer: "AWS"
      date: "Sep, 2024"
    - name: Microsoft Certified Azure Data Engineer Associate
      url:
      issuer: "Microsoft"
      date: "Jun, 2024"
    - name: Databricks Certified Data Engineer Associate
      url:
      issuer: "Databricks"
      date: "Mar, 2023"
    - name: PingCAP Certified TiDB Professional
      url:
      issuer: "PingCAP"
      date: "Mar, 2022"
  projects:
    - name: Bayer Customer Data Platform (CDP)
      url:
      description: Cloud-native data lakehouse unifying customer data across the enterprise.
      startDate: Dec 2022
      endDate:
      summary: |
        - Architected a serverless ETL architecture using AWS Glue and Lambda, optimizing compute costs by 40% while maintaining strict SLAs for data freshness.
        - Established a robust Infrastructure as Code (IaC) foundation using Terraform and GitHub Actions, reducing deployment time by 60% and ensuring 99.9% infrastructure uptime.
        - Designed an event-driven orchestration workflow with AWS Step Functions and EventBridge to manage complex dependencies across MySQL, SQL Server, and PostgreSQL.
      keywords:
        - "AWS (Glue, Lambda, Step Functions)"
        - "Terraform"
        - "GitHub Actions"
        - "Data Lakehouse"
    - name: Enterprise Data Warehouse Migration to AWS
      url:
      description: Strategic migration of a 10TB+ legacy warehouse to Amazon Redshift.
      startDate: Nov 2022
      endDate: May 2022
      summary: |
        - Developed a custom Python migration engine (`dm4`) using AST parsing (`sqlglot`) to achieve 99% automated DDL translation accuracy for over 2,000 tables.
        - Engineered a high-throughput stream processing pipeline using UNIX named pipes and `bcp`, accelerating 10TB data transfer by 300% compared to traditional staging.
        - Designed performant Redshift schemas with optimal Distribution and Sort Keys, resulting in a 5x improvement in complex analytical query performance.
      keywords:
        - "Amazon Redshift"
        - "Python"
        - "Database Migration"
        - "Performance Tuning"
    - name: Retail Master Data Management Platform
      url:
      description: Centralized retail data governance system on Microsoft Power Platform.
      startDate: May 2025
      endDate: Dec 2025
      summary: |
        - Designed a billion-scale storage layer using Azure Cosmos DB, ensuring sub-second response times for current state queries while maintaining full historical versioning.
        - Implemented a cross-cloud data synchronization bridge between Azure Dataverse and AWS analytical warehouses using Synapse Link and custom adapters.
      keywords:
        - "Azure Cosmos DB"
        - "Power Platform"
        - "Data Governance"
        - "Cross-Cloud Integration"
